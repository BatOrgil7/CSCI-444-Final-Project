print(summary_stats)
library(rvest)
library(dplyr)
library(stringr)
library(purrr)
# Enhanced scraping function with more data
scrape_detailed_salaries <- function(state_abbr = "AL") {
tryCatch({
base_url <- "https://www.onetonline.org/link/localwages/15-1252.00"
url <- paste0(base_url, "?st=", state_abbr)
page <- read_html(url)
# Extract multiple tables if available
tables <- page %>% html_nodes("table.report_table")
# Main salary table (usually the first one)
if (length(tables) > 0) {
salary_data <- tables[[1]] %>% html_table()
# Clean column names
if (ncol(salary_data) >= 7) {
colnames(salary_data) <- c("Area", "Employment", "Percentile_10",
"Percentile_25", "Median", "Percentile_75",
"Percentile_90")
# Clean and format data
salary_data_clean <- salary_data %>%
mutate(
across(-Area, ~str_remove_all(.x, "[$,]")),
across(-Area, as.numeric),
Area = str_trim(Area),
State = state_abbr,
Scraped_Date = Sys.Date()
)
return(salary_data_clean)
}
}
return(NULL)
}, error = function(e) {
message("Error scraping ", state_abbr, ": ", e$message)
return(NULL)
})
}
# Scrape all states
all_states <- c("AL", "AK", "AZ", "AR", "CA", "CO", "CT", "DE", "FL", "GA",
"HI", "ID", "IL", "IN", "IA", "KS", "KY", "LA", "ME", "MD",
"MA", "MI", "MN", "MS", "MO", "MT", "NE", "NV", "NH", "NJ",
"NM", "NY", "NC", "ND", "OH", "OK", "OR", "PA", "RI", "SC",
"SD", "TN", "TX", "UT", "VT", "VA", "WA", "WV", "WI", "WY")
# Scrape with progress bar and delays
library(pbapply)
all_salary_data <- pblapply(all_states, function(state) {
Sys.sleep(2)  # Longer delay to be respectful
scrape_detailed_salaries(state)
})
# Enhanced scraping function with more data
scrape_detailed_salaries <- function(state_abbr = "AL") {
tryCatch({
base_url <- "https://www.onetonline.org/link/localwages/15-1252.00"
url <- paste0(base_url, "?st=", state_abbr)
page <- read_html(url)
# Extract multiple tables if available
tables <- page %>% html_nodes("table.report_table")
# Main salary table (usually the first one)
if (length(tables) > 0) {
salary_data <- tables[[1]] %>% html_table()
# Clean column names
if (ncol(salary_data) >= 7) {
colnames(salary_data) <- c("Area", "Employment", "Percentile_10",
"Percentile_25", "Median", "Percentile_75",
"Percentile_90")
# Clean and format data
salary_data_clean <- salary_data %>%
mutate(
across(-Area, ~str_remove_all(.x, "[$,]")),
across(-Area, as.numeric),
Area = str_trim(Area),
State = state_abbr,
Scraped_Date = Sys.Date()
)
return(salary_data_clean)
}
}
return(NULL)
}, error = function(e) {
message("Error scraping ", state_abbr, ": ", e$message)
return(NULL)
})
}
# Scrape all states
all_states <- c("AL", "AK", "AZ", "AR", "CA", "CO", "CT", "DE", "FL", "GA",
"HI", "ID", "IL", "IN", "IA", "KS", "KY", "LA", "ME", "MD",
"MA", "MI", "MN", "MS", "MO", "MT", "NE", "NV", "NH", "NJ",
"NM", "NY", "NC", "ND", "OH", "OK", "OR", "PA", "RI", "SC",
"SD", "TN", "TX", "UT", "VT", "VA", "WA", "WV", "WI", "WY")
# Scrape with progress indication (no external packages needed)
all_salary_data <- list()
for (i in seq_along(all_states)) {
state <- all_states[i]
cat("Scraping", state, "-", i, "of", length(all_states), "\n")
# Add delay to be respectful to the server
Sys.sleep(2)
result <- scrape_detailed_salaries(state)
if (!is.null(result)) {
all_salary_data[[i]] <- result
}
}
# Combine all data
final_salary_data <- bind_rows(all_salary_data) %>%
filter(!is.na(Median))  # Remove rows with no median salary
# Install required packages
install.packages(c("dplyr", "purrr", "readr", "httr"))
# Load libraries
library(dplyr)
library(purrr)
library(readr)
library(httr)
# Enhanced version with better column handling
download_state_salaries_enhanced <- function(state_abbr = "AL") {
tryCatch({
csv_url <- paste0("https://www.onetonline.org/link/localwages/15-1252.00?st=",
state_abbr, "&format=csv")
cat("Downloading", state_abbr, "\n")
# Use httr for more robust downloading
if (!requireNamespace("httr", quietly = TRUE)) {
install.packages("httr")
}
library(httr)
response <- GET(csv_url)
if (status_code(response) == 200) {
# Read CSV content directly from response
content <- content(response, "text", encoding = "UTF-8")
# Read CSV, handling potential header issues
state_data <- read_csv(content, skip = 1, show_col_types = FALSE)
# Standardize column names
if (ncol(state_data) > 0) {
# Common column name patterns in O*NET data
col_names <- names(state_data)
# Add state identifier
state_data$State <- state_abbr
state_data$State_Name <- state.name[match(state_abbr, state.abb)]
return(state_data)
}
} else {
cat("HTTP error", status_code(response), "for", state_abbr, "\n")
}
return(NULL)
}, error = function(e) {
cat("Error with", state_abbr, ":", e$message, "\n")
return(NULL)
})
}
# Download all states
all_enhanced_data <- list()
for (state in all_states) {
state_data <- download_state_salaries_enhanced(state)
if (!is.null(state_data) && nrow(state_data) > 0) {
all_enhanced_data[[state]] <- state_data
}
Sys.sleep(1)  # Polite delay
}
# Combine and save
if (length(all_enhanced_data) > 0) {
combined_enhanced <- bind_rows(all_enhanced_data)
# Clean column names
names(combined_enhanced) <- make.names(names(combined_enhanced))
write_csv(combined_enhanced, "data/software_engineer_salaries_all_states.csv")
cat("Successfully downloaded", length(all_enhanced_data), "states\n")
cat("Total records:", nrow(combined_enhanced), "\n")
# Summary by state
summary_by_state <- combined_enhanced %>%
group_by(State, State_Name) %>%
summarise(
Regions = n(),
.groups = 'drop'
)
print(summary_by_state)
} else {
cat("No data was successfully downloaded\n")
}
View(all_enhanced_data)
# Load required libraries
library(rvest)
library(dplyr)
library(purrr)
library(stringr)
# Function to scrape HTML table for a state
scrape_state_table <- function(state_abbr = "AL") {
tryCatch({
url <- paste0("https://www.onetonline.org/link/localwages/15-1252.00?st=", state_abbr)
cat("Scraping", state_abbr, "\n")
# Read the page
page <- read_html(url)
# Target the specific table with the classes you mentioned
table_node <- page %>%
html_node("table.tablesorter.tablesorter-bootstrap.table-responsive-md.w-100.table-fluid")
if (!is.null(table_node)) {
# Extract the table
salary_table <- html_table(table_node, fill = TRUE)
# Add state information
salary_table$State <- state_abbr
salary_table$State_Name <- state.name[match(state_abbr, state.abb)]
return(salary_table)
} else {
cat("Table not found for", state_abbr, "\n")
return(NULL)
}
}, error = function(e) {
cat("Error scraping", state_abbr, ":", e$message, "\n")
return(NULL)
})
}
# All US states
all_states <- c("AL", "AK", "AZ", "AR", "CA", "CO", "CT", "DE", "FL", "GA",
"HI", "ID", "IL", "IN", "IA", "KS", "KY", "LA", "ME", "MD",
"MA", "MI", "MN", "MS", "MO", "MT", "NE", "NV", "NH", "NJ",
"NM", "NY", "NC", "ND", "OH", "OK", "OR", "PA", "RI", "SC",
"SD", "TN", "TX", "UT", "VT", "VA", "WA", "WV", "WI", "WY")
# Scrape all states
html_tables <- list()
for (i in seq_along(all_states)) {
state <- all_states[i]
cat("Processing", i, "of", length(all_states), "-", state, "\n")
table_data <- scrape_state_table(state)
if (!is.null(table_data) && nrow(table_data) > 0) {
html_tables[[state]] <- table_data
}
Sys.sleep(1)  # Polite delay
}
View(summary_by_state)
View(summary_by_state)
View(state_data)
# Combine all tables
if (length(html_tables) > 0) {
combined_html_data <- bind_rows(html_tables)
cat("Successfully scraped", length(html_tables), "states\n")
cat("Total records:", nrow(combined_html_data), "\n")
# View the structure
cat("\nData structure:\n")
print(str(combined_html_data))
# Save the data
write_csv(combined_html_data, "data/software_salaries_html_tables.csv")
cat("Data saved to 'data/software_salaries_html_tables.csv'\n")
} else {
cat("No HTML tables were successfully scraped\n")
}
# Load the data
salaries <- read.csv("software_salaries_html_tables.csv", stringsAsFactors = FALSE)
# Load the data
salaries <- read.csv("data/software_salaries_html_tables.csv", stringsAsFactors = FALSE)
# Clean the salary columns (remove $, commas, and convert to numeric)
clean_salaries <- salaries %>%
mutate(
across(c(Annual.Low..10., Annual.QL..25., Annual.Median..50.,
Annual.QU..75., Annual.High..90.),
~ as.numeric(gsub("[$,]", "", .))),
# Handle the "$239,200+" values by taking the minimum value
Annual.High..90. = ifelse(Annual.High..90. > 250000, 239200, Annual.High..90.)
)
# Load required libraries
library(dplyr)
library(tidyr)
library(stringr)
library(knitr)
# Read the data
salaries <- read.csv("software_salaries_html_tables.csv", stringsAsFactors = FALSE)
# Read the data
salaries <- read.csv("data/software_salaries_html_tables.csv", stringsAsFactors = FALSE)
# Clean the data
cleaned_salaries <- salaries %>%
# Remove dollar signs, commas, and convert to numeric
mutate(
across(c(Annual.Low..10., Annual.QL..25., Annual.Median..50.,
Annual.QU..75., Annual.High..90.),
~ as.numeric(gsub("[$,]", "", .))),
# Clean location names
Location = str_trim(Location),
# Add region type classification
Region_Type = case_when(
Location == "United States" ~ "National",
Location == State_Name ~ "Statewide",
grepl("nonmetropolitan", Location, ignore.case = TRUE) ~ "Non-metropolitan",
TRUE ~ "Metropolitan"
)
) %>%
# Rename columns to be more readable
rename(
Location_Type = Region_Type,
P10 = Annual.Low..10.,
P25 = Annual.QL..25.,
Median = Annual.Median..50.,
P75 = Annual.QU..75.,
P90 = Annual.High..90.
) %>%
# Reorder columns logically
select(Location, Location_Type, State, State_Name, P10, P25, Median, P75, P90)
# First, let's check the actual column names in your data
cat("Actual column names in the dataset:\n")
print(names(salaries))
# Check the first few rows to see the data structure
cat("\nFirst few rows of the data:\n")
print(head(salaries))
# Check the structure of the data
cat("\nData structure:\n")
str(salaries)
# Load required libraries
library(dplyr)
library(tidyr)
library(stringr)
library(knitr)
# Read the data
salaries <- read.csv("data/software_salaries_html_tables.csv", stringsAsFactors = FALSE)
# Clean the data with correct column names
cleaned_salaries <- salaries %>%
# Remove dollar signs, commas, and convert to numeric
mutate(
P10 = as.numeric(gsub("[$,]", "", Annual.Low..10..)),
P25 = as.numeric(gsub("[$,]", "", Annual.QL..25..)),
Median = as.numeric(gsub("[$,]", "", Annual.Median..50..)),
P75 = as.numeric(gsub("[$,]", "", Annual.QU..75..)),
P90 = as.numeric(gsub("[$,]", "", Annual.High..90..)),
# Clean location names
Location = str_trim(Location),
# Add region type classification
Location_Type = case_when(
Location == "United States" ~ "National",
Location == State_Name ~ "Statewide",
grepl("nonmetropolitan", Location, ignore.case = TRUE) ~ "Non-metropolitan",
TRUE ~ "Metropolitan"
)
) %>%
# Select and reorder columns
select(Location, Location_Type, State, State_Name, P10, P25, Median, P75, P90)
# View cleaned data structure
cat("Cleaned Data Structure:\n")
str(cleaned_salaries)
cat("\nFirst 10 rows of cleaned data:\n")
head(cleaned_salaries, 10) %>% kable(digits = 0)
statewide_summary <- cleaned_salaries %>%
filter(Location_Type == "Statewide") %>%
select(State, State_Name, P10, P25, Median, P75, P90) %>%
arrange(desc(Median))
cat("Table 1: Statewide Software Engineering Salaries (Ordered by Median)\n")
kable(statewide_summary, digits = 0)
View(cleaned_salaries)
library(tidyverse)
library(kableExtra)
library(scales)
library(usmap)
library(leaflet)
library(maps)
# Read salary data
salaries_raw <- read_csv("data/software_salaries_html_tables.csv", show_col_types = FALSE)
# Clean and prepare salary table
cleaned_salaries <- salaries_raw %>%
mutate(
P10    = as.numeric(gsub("[$,]", "", Annual.Low..10..)),
P25    = as.numeric(gsub("[$,]", "", Annual.QL..25..)),
Median = as.numeric(gsub("[$,]", "", Annual.Median..50..)),
P75    = as.numeric(gsub("[$,]", "", Annual.QU..75..)),
P90    = as.numeric(gsub("[$,]", "", Annual.High..90..)),
Location      = str_trim(Location),
Location_Type = case_when(
Location == "United States" ~ "National",
Location == State_Name      ~ "Statewide",
str_detect(Location, regex("nonmetropolitan", ignore_case = TRUE)) ~ "Non-metropolitan",
TRUE                        ~ "Metropolitan"
)
) %>%
select(Location, Location_Type, State, State_Name, P10, P25, Median, P75, P90)
---
title: "Final Project - Question 2 Section"
statewide_summary <- cleaned_salaries %>%
filter(Location_Type == "Statewide") %>%
select(State, State_Name, P10, P25, Median, P75, P90) %>%
mutate(state = State) %>%        # add lowercase 'state' column for usmap
arrange(desc(Median))
cat("Table 1: Statewide Software Engineering Salaries (Ordered by Median)\n")
kable(statewide_summary, digits = 0)
library(rvest)
library(dplyr)
library(purrr)
library(stringr)
# Function to scrape HTML table for a state
scrape_state_table <- function(state_abbr = "AL") {
tryCatch({
url <- paste0("https://www.onetonline.org/link/localwages/15-1252.00?st=", state_abbr)
cat("Scraping", state_abbr, "\n")
# Read the page
page <- read_html(url)
# Target the specific table with the classes you mentioned
table_node <- page %>%
html_node("table.tablesorter.tablesorter-bootstrap.table-responsive-md.w-100.table-fluid")
if (!is.null(table_node)) {
# Extract the table
salary_table <- html_table(table_node, fill = TRUE)
# Add state information
salary_table$State <- state_abbr
salary_table$State_Name <- state.name[match(state_abbr, state.abb)]
return(salary_table)
} else {
cat("Table not found for", state_abbr, "\n")
return(NULL)
}
}, error = function(e) {
cat("Error scraping", state_abbr, ":", e$message, "\n")
return(NULL)
})
}
# All US states
all_states <- c("AL", "AK", "AZ", "AR", "CA", "CO", "CT", "DE", "FL", "GA",
"HI", "ID", "IL", "IN", "IA", "KS", "KY", "LA", "ME", "MD",
"MA", "MI", "MN", "MS", "MO", "MT", "NE", "NV", "NH", "NJ",
"NM", "NY", "NC", "ND", "OH", "OK", "OR", "PA", "RI", "SC",
"SD", "TN", "TX", "UT", "VT", "VA", "WA", "WV", "WI", "WY")
# Scrape all states
html_tables <- list()
for (i in seq_along(all_states)) {
state <- all_states[i]
cat("Processing", i, "of", length(all_states), "-", state, "\n")
table_data <- scrape_state_table(state)
if (!is.null(table_data) && nrow(table_data) > 0) {
html_tables[[state]] <- table_data
}
Sys.sleep(1)  # Polite delay
}
# Combine all tables
if (length(html_tables) > 0) {
combined_html_data <- bind_rows(html_tables)
cat("Successfully scraped", length(html_tables), "states\n")
cat("Total records:", nrow(combined_html_data), "\n")
# View the structure
cat("\nData structure:\n")
print(str(combined_html_data))
# Save the data
write_csv(combined_html_data, "data/software_salaries_html_tables.csv")
cat("Data saved to 'data/software_salaries_html_tables.csv'\n")
} else {
cat("No HTML tables were successfully scraped\n")
}
# Load required libraries
library(dplyr)
library(tidyr)
library(stringr)
library(knitr)
# Read the data
salaries <- read.csv("data/software_salaries_html_tables.csv", stringsAsFactors = FALSE)
# Clean the data with correct column names
cleaned_salaries <- salaries %>%
# Remove dollar signs, commas, and convert to numeric
mutate(
P10 = as.numeric(gsub("[$,]", "", Annual.Low..10..)),
P25 = as.numeric(gsub("[$,]", "", Annual.QL..25..)),
Median = as.numeric(gsub("[$,]", "", Annual.Median..50..)),
P75 = as.numeric(gsub("[$,]", "", Annual.QU..75..)),
P90 = as.numeric(gsub("[$,]", "", Annual.High..90..)),
# Clean location names
Location = str_trim(Location),
# Add region type classification
Location_Type = case_when(
Location == "United States" ~ "National",
Location == State_Name ~ "Statewide",
grepl("nonmetropolitan", Location, ignore.case = TRUE) ~ "Non-metropolitan",
TRUE ~ "Metropolitan"
)
) %>%
# Select and reorder columns
select(Location, Location_Type, State, State_Name, P10, P25, Median, P75, P90)
# View cleaned data structure
cat("Cleaned Data Structure:\n")
str(cleaned_salaries)
cat("\nFirst 10 rows of cleaned data:\n")
head(cleaned_salaries, 10) %>% kable(digits = 0)
statewide_summary <- cleaned_salaries %>%
filter(Location_Type == "Statewide") %>%
select(State, State_Name, P10, P25, Median, P75, P90) %>%
mutate(state = State) %>%        # add lowercase 'state' column for usmap
arrange(desc(Median))
cat("Table 1: Statewide Software Engineering Salaries (Ordered by Median)\n")
kable(statewide_summary, digits = 0)
